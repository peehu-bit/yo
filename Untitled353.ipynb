{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled353.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUBLj3n7HiEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from sklearn.decomposition.pca import PCA\n",
        "from sklearn.linear_model.logistic import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import warnings\n",
        "\n",
        "start_time = time.clock()\n",
        "\n",
        "\n",
        "#8381 of the images are RGB, so they will need to be converted to greyscale.\n",
        "def rgb_to_gray(img):\n",
        "    if len(img.shape) == 2: #already gray\n",
        "        return(img)\n",
        "    grayImage = np.zeros(img.shape)\n",
        "    #These scaling coefficients are evidently standard for RGB->Grayscale conversion\n",
        "    R = img[:,:,0]*0.299\n",
        "    G = img[:,:,1]*0.587\n",
        "    B = img[:,:,2]*0.114\n",
        "    \n",
        "    grayImage = R + G + B\n",
        "    \n",
        "    return(grayImage)\n",
        "    \n",
        "#We will also need to group together nearby pixels in order to \n",
        "#reduce image complexity and uniformize image size (so that every image has\n",
        "#the same number of features).\n",
        "    \n",
        "def img_compress(img, x_bins=100,y_bins=100):\n",
        "    x_splits = np.linspace(0,img.shape[1]-1,x_bins+1, dtype = int)\n",
        "    y_splits=  np.linspace(0,img.shape[0]-1,y_bins+1, dtype = int)\n",
        "    \n",
        "    compressed = np.zeros((y_bins,x_bins))\n",
        "    \n",
        "    for i in range(y_bins):\n",
        "        for j in range(x_bins):\n",
        "            with warnings.catch_warnings():\n",
        "                 warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "                 temp = np.mean(img[y_splits[i]:y_splits[i+1],\n",
        "                                      x_splits[j]:x_splits[j+1]])\n",
        "            if math.isnan(temp):\n",
        "                if y_splits[i]==y_splits[i+1]:\n",
        "                    compressed[i,j]=compressed[i-1,j]\n",
        "                else:\n",
        "                    compressed[i,j] = compressed[i,j-1]\n",
        "            else:\n",
        "                compressed[i,j] = int(temp)\n",
        "    return(compressed)\n",
        "\n",
        "\n",
        "train_dir = '../input/train'\n",
        "test_dir = '../input/test'\n",
        "#Convert .jpg images into pixel arrays\n",
        "#imgs_train = [rgb_to_gray(mpimg.imread(train_dir + '/' + file, format = 'JPG')) for file in os.listdir(train_dir)]\n",
        "imgs_train_1 = [cv2.imread(train_dir + '/' + file) for file in os.listdir(train_dir)]\n",
        "imgs_train = [cv2.cvtColor( img, cv2.COLOR_RGB2GRAY ) for img in imgs_train_1]\n",
        "#rows_train = [img.shape[0] for img in imgs_train]\n",
        "#cols_train = [img.shape[1] for img in imgs_train]\n",
        "print(\"Training images read.\", \"Time:\", time.clock()-start_time)\n",
        "\n",
        "\n",
        "min_cols = 1024\n",
        "min_rows = 683\n",
        "\n",
        "\n",
        "\n",
        "#Get indices of those pictures which aren't too small\n",
        "good_pics = [i for i in range(len(imgs_train)) if (imgs_train[i].shape[0]>=min_rows)and(imgs_train[i].shape[1]>=min_cols)]\n",
        "#Select only pictures that are sufficiently large\n",
        "imgs_train = [imgs_train[i] for i in good_pics]\n",
        "\n",
        "\n",
        "#Compress images. Unfortunately we have to distort the aspect ratio, which may\n",
        "#lose valuable information. But if we do not do this then features will not\n",
        "#have consistent meaning across pictures, even if we ensure that all pictures\n",
        "#get compressed to the same resolution (pixel count)\n",
        "\n",
        "#We need to find the smallest dimension across all images (train and test) in\n",
        "#order to properly compress; otherwise the compressing algorithm will generate\n",
        "#NaN-values.\n",
        "\n",
        "compressed_train_imgs = [img_compress(img, min_cols, min_rows) for img in imgs_train]\n",
        "print(\"Training images compressed.\", \"Time:\", time.clock()-start_time)\n",
        "        \n",
        "del imgs_train\n",
        "\n",
        "\n",
        "\n",
        "#Extract filenames to later associate to landmark IDs\n",
        "filenames = [file for file in os.listdir(train_dir)]\n",
        "filenames = [filenames[i] for i in good_pics]\n",
        "filenames_test = [file for file in os.listdir(test_dir)]\n",
        "#Convert images into pandas dataframe format for learning the model\n",
        "df1 = pd.DataFrame(data = [img.ravel() for img in compressed_train_imgs])\n",
        "df2 = pd.DataFrame(data = filenames, columns = ['FileName'])\n",
        "data1 = pd.concat([df2,df1],axis = 1)\n",
        "#Obtain filenames, indexed by landmark IDs\n",
        "data2 = pd.read_csv('./train.csv',names = ['FileName','Landmark'])\n",
        "\n",
        "#Map whale IDs to images via filenames (index of data1 is matched to value of data2)\n",
        "data = data2.merge(data1, on = 'FileName')\n",
        "data = data.drop('FileName',axis = 1)\n",
        "\n",
        "del compressed_train_imgs, df1, df2, data1, data2\n",
        "\n",
        "#Extract training examples and labels, and get test set.\n",
        "X_train = data.iloc[:,1:]\n",
        "y_train = data['Landmark']\n",
        "\n",
        "\n",
        "#Perform principal component analysis for dimensionality reduction\n",
        "#Try playing around with n_components to get better scores\n",
        "\n",
        "pca = PCA(random_state = 42, n_components = 100, whiten = True)\n",
        "pca.fit(X_train)\n",
        "print(\"PCA fitting complete\", \"Time:\", time.clock()-start_time)\n",
        "\n",
        "#Try other classifiers for better scores.\n",
        "logreg = LogisticRegression(C = 1e-2)\n",
        "\n",
        "#Define our pipeline. First we transform the data into its principal components,\n",
        "#then learn the logistic regression model\n",
        "clf = Pipeline([('pca',pca),\n",
        "                ('logreg',logreg),\n",
        "                ])\n",
        "\n",
        "#Fit model\n",
        "clf.fit(X_train,y_train)\n",
        "print('Estimator fitting complete.', \"Time:\", time.clock()-start_time)\n",
        "#Score model on training data\n",
        "print(\"Score on training set:\", clf.score(X_train,y_train))\n",
        "#We no longer need the training data. Delete to free up memory.\n",
        "del data, X_train, y_train\n",
        "\n",
        "#Read and clean test set\n",
        "#imgs_test = [rgb_to_gray(mpimg.imread(test_dir + '/' + file)) for file in os.listdir(test_dir)]\n",
        "imgs_test_1 = [cv2.imread(test_dir + '/' + file) for file in os.listdir(test_dir)]\n",
        "imgs_test = [cv2.cvtColor( img, cv2.COLOR_RGB2GRAY ) for img in imgs_test_1]\n",
        "\n",
        "print(\"Test images read.\", \"Time:\", time.clock()-start_time)\n",
        "#Smallest number of columns and rows across all test pictures\n",
        "compressed_test_imgs = [img_compress(img, min_cols, min_rows) for img in imgs_test]\n",
        "print(\"Test images compressed.\", \"Time:\", time.clock()-start_time)\n",
        "del imgs_test\n",
        "X_test = pd.DataFrame([img.ravel() for img in compressed_test_imgs])\n",
        "del compressed_test_imgs\n",
        "\n",
        "#Extract the top 5 predictions for each test example\n",
        "\n",
        "y_preds = clf.predict_proba(X_test)\n",
        "print(\"Predictions made on test set.\", \"Time:\", time.clock()-start_time)\n",
        "\n",
        "\n",
        "#Extract top 5 results\n",
        "results = pd.DataFrame(data = [clf.classes_[np.argsort(y_preds[i,:])[-5:]] for i in range(y_preds.shape[0])],\n",
        "                       index = filenames_test)\n",
        "def list_to_str(L):\n",
        "    string = \"\"\n",
        "    for word in L:\n",
        "        string =  word + \" \"         \n",
        "    return(string)\n",
        "        \n",
        "results2 = pd.DataFrame(data = [list_to_str(results.iloc[i].values) for i in range(results.shape[0])],\n",
        "                        index = filenames_test,columns = ['Landmark'])\n",
        " \n",
        "results2.to_csv('results.csv', sep = ',', index_label = 'Image', header = True)\n",
        "\n",
        "print(\"Done.\", \"Time:\", time.clock()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}